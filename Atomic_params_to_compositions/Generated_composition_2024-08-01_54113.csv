training_data,C:\Users\ezxac5\Documents\GitHub\Materials-Discovery\Machine_learning\Active_learning\Pyrochlore_database_atomic_param_unnormalized.csv
Iterations,100000
YbA,0.24
SmA,0.36
GdA,0.32
EuA,0.08
HfB,0.058823529411764705
TiB,0.4117647058823529
ZrB,0.058823529411764705
CeB,0.058823529411764705
SnB,0.4117647058823529
Distance,0.03706102898070173
RA,1.0470800005134588
RB,0.6685294119582286
MA,158.13632004504566
MB,92.69817650195753
Entropy_conf,2.3965852640689707
Distance,0.03706102898070173
0,Target parameters
RA,1.04575
RB,0.66
MA,158.1535
MB,91.362
Entropy_conf,2.376727
0,Constraints
num_A,4
num_B,5
LaA,1
NdA,1
SmA,1
GdA,1
EuA,1
TmA,1
YbA,1
YA,1
DyA,1
HoA,1
ErA,1
PrA,1
CeA,1
ZrB,1
HfB,1
SnB,1
CeB,1
TiB,1
0,Surrogate Model Parameters
Model_name,model_linear_RA
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_linear_RB
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_linear_MA
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_linear_MB
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_svr_entropy
C,1.0
cache_size,200
coef0,0.0
degree,1
epsilon,0.001
gamma,scale
kernel,rbf
max_iter,-1
shrinking,True
tol,0.001
verbose,False
0,Normalization parameters
RA,0.985,1.16
RB,0.605,0.87
MA,88.90585,173.04
MB,47.867,134.857
Entropy_conf,0.0,3.0
0,Top 10 compositions
,YbA,SmA,GdA,EuA,HfB,TiB,ZrB,CeB,SnB,Distance,HoA,LaA,ErA,NdA,CeA,DyA,TmA,PrA
0,0.24,0.36,0.32,0.08,0.058823529411764705,0.4117647058823529,0.058823529411764705,0.058823529411764705,0.4117647058823529,0.03706102898070173,,,,,,,,
1,0.13043478260869565,,0.391304347826087,,0.10526315789473684,0.47368421052631576,0.15789473684210525,0.05263157894736842,0.21052631578947367,0.052885384296210904,0.30434782608695654,0.17391304347826086,,,,,,
2,0.25,,0.4166666666666667,,0.15384615384615385,0.46153846153846156,0.15384615384615385,0.07692307692307693,0.15384615384615385,0.05392575149075696,,0.16666666666666666,0.16666666666666666,,,,,
3,0.4090909090909091,0.2727272727272727,0.22727272727272727,,0.13333333333333333,0.5333333333333333,0.06666666666666667,0.06666666666666667,0.2,0.05412555720480671,,0.09090909090909091,,,,,,
4,0.2916666666666667,,0.20833333333333334,,0.058823529411764705,0.4117647058823529,0.17647058823529413,0.058823529411764705,0.29411764705882354,0.05473586216207212,0.125,,,0.375,,,,
5,,0.36,,,0.1,0.45,0.15,0.05,0.25,0.05613881565504718,0.16,,0.36,,0.12,,,
6,0.3333333333333333,0.19047619047619047,,,0.14285714285714285,0.42857142857142855,0.2857142857142857,0.047619047619047616,0.09523809523809523,0.056985269605812384,,0.19047619047619047,,,,0.2857142857142857,,
7,0.125,0.2916666666666667,0.3333333333333333,,0.08333333333333333,0.375,0.16666666666666666,0.041666666666666664,0.3333333333333333,0.0601155270490556,,,,,,0.25,,
8,,,,0.2962962962962963,0.14285714285714285,0.42857142857142855,0.23809523809523808,0.047619047619047616,0.14285714285714285,0.060939028401612194,,,,0.2222222222222222,,0.25925925925925924,0.2222222222222222,
9,0.3333333333333333,,,,0.04,0.36,0.2,0.04,0.36,0.06177799903686569,0.19047619047619047,,,,0.38095238095238093,,,0.09523809523809523
