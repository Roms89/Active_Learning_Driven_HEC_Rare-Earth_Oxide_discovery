training_data,C:\Users\ezxac5\Documents\GitHub\Materials-Discovery\Machine_learning\Active_learning\Pyrochlore_database_atomic_param_unnormalized.csv
Iterations,100000
EuA,0.2916666666666667
SmA,0.3333333333333333
YbA,0.3333333333333333
LaA,0.041666666666666664
TiB,0.47058823529411764
HfB,0.23529411764705882
ZrB,0.29411764705882354
Distance,0.03881503855587196
RA,1.0472500004820227
RB,0.6635294116569734
MA,157.9105613120368
MB,91.35388233895475
Entropy_conf,2.2707710106303325
Distance,0.03881503855587196
0,Target parameters
RA,1.04575
RB,0.66
MA,158.1535
MB,91.362
Entropy_conf,2.376727
0,Constraints
num_A,4
num_B,3
LaA,1
NdA,1
SmA,1
GdA,1
EuA,1
TmA,1
YbA,1
YA,1
DyA,1
HoA,1
ErA,1
PrA,1
CeA,1
ZrB,1
HfB,1
SnB,1
CeB,1
TiB,1
0,Surrogate Model Parameters
Model_name,model_linear_RA
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_linear_RB
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_linear_MA
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_linear_MB
copy_X,True
fit_intercept,True
n_jobs,
positive,False
Model_name,model_svr_entropy
C,1.0
cache_size,200
coef0,0.0
degree,1
epsilon,0.001
gamma,scale
kernel,rbf
max_iter,-1
shrinking,True
tol,0.001
verbose,False
0,Normalization parameters
RA,0.985,1.16
RB,0.605,0.87
MA,88.90585,173.04
MB,47.867,134.857
Entropy_conf,0.0,3.0
0,Top 10 compositions
,EuA,SmA,YbA,LaA,TiB,HfB,ZrB,Distance,GdA,DyA,SnB,TmA,CeA,ErA,NdA,HoA
0,0.2916666666666667,0.3333333333333333,0.3333333333333333,0.041666666666666664,0.47058823529411764,0.23529411764705882,0.29411764705882354,0.03881503855587196,,,,,,,,
1,,,0.24,0.16,0.3684210526315789,,0.15789473684210525,0.046964054899586494,0.36,0.24,0.47368421052631576,,,,,
2,,0.2692307692307692,0.19230769230769232,,0.375,,0.125,0.05081075160115203,,,0.5,0.3076923076923077,0.23076923076923078,,,
3,,,0.3181818181818182,,0.5,0.2777777777777778,0.2222222222222222,0.05787421359124829,0.4090909090909091,,,,0.22727272727272727,0.045454545454545456,,
4,0.29411764705882354,,0.35294117647058826,,0.5,0.16666666666666666,,0.05837544833527658,0.11764705882352941,,0.3333333333333333,,,,0.23529411764705882,
5,,,0.14285714285714285,0.19047619047619047,0.3157894736842105,,0.2631578947368421,0.06053560844204058,0.23809523809523808,,0.42105263157894735,,,,,0.42857142857142855
6,,,,,0.2777777777777778,,0.2222222222222222,0.061771534788012734,0.25,,0.5,,0.1875,0.25,,0.3125
7,0.21428571428571427,0.14285714285714285,0.2857142857142857,,0.35714285714285715,,0.07142857142857142,0.06180927883307606,0.35714285714285715,,0.5714285714285714,,,,,
8,0.1,0.4,0.2,,0.2857142857142857,,0.2857142857142857,0.06355875387451897,,0.3,0.42857142857142855,,,,,
9,,,,0.17391304347826086,0.5,0.2222222222222222,0.2777777777777778,0.06453449219780039,0.30434782608695654,,,0.2608695652173913,,,,0.2608695652173913
